# Electricity Theft Detection via Hybrid SLM + Time-Series Anomaly Detection

[![CI](https://github.com/yourusername/electricity-theft-detection/actions/workflows/ci.yml/badge.svg)](https://github.com/yourusername/electricity-theft-detection/actions/workflows/ci.yml)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Production-ready system for detecting anomalous electricity consumption patterns indicative of energy theft, combining statistical time-series analysis with Small Language Models for interpretable reporting.

## üèóÔ∏è Architecture

This system addresses the **critical weaknesses** of naive "text-as-input" approaches by using a **hybrid architecture**:

| Component | Technology | Role |
|-----------|-----------|------|
| **Feature Extraction** | TSFRESH | Extracts 700+ statistical features (trend, seasonality, entropy) |
| **Anomaly Detection** | Isolation Forest | Unsupervised scoring for novel theft patterns |
| **Classification** | XGBoost | High-precision fraud detection with IF features as input |
| **Reasoning Engine** | Phi-3/Llama-3-8B (4-bit GGUF) | Natural language investigation reports from structured outputs |

**Why this architecture?**
- ‚ùå **Avoids**: Token inflation from "Day 1: 12kWh..." textification (inefficient, lossy)
- ‚úÖ **Uses**: Structured feature vectors ‚Üí Statistical detection ‚Üí LLM interpretation
- üéØ **Result**: 10x faster inference, higher accuracy, interpretable outputs

## üöÄ Quick Start

### Prerequisites
- Python 3.10+
- 8GB+ RAM (16GB recommended for training)
- For edge deployment: 4GB RAM with quantized models

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/electricity-theft-detection.git
cd electricity-theft-detection

# Install dependencies
poetry install

# Download SGCC dataset (manual download from Kaggle required due to licensing)
# Place in data/raw/sgcc.csv

# Download quantized SLM (automatic)
poetry run python scripts/download_quantized_model.py --model phi-3-mini
```

### Training

```bash
# Full pipeline: Data ‚Üí Features ‚Üí Model ‚Üí Quantization
poetry run python scripts/train_models.py \
  --config config/pipeline_config.yaml \
  --output models/

# Or step-by-step
poetry run python -m src.pipeline.data_pipeline --extract-tsfresh
poetry run python scripts/train_models.py --train-hybrid
poetry run python scripts/quantize_llm.py
```
### Inference

```bash
# Start the full stack (API + Redis + Workers)
docker-compose up -d

# Or manually
redis-server &
celery -A src.workers.tasks worker --loglevel=info -Q math_queue,llm_queue &
uvicorn src.api.main:app --port 8000
```

# üìä Dataset

### SGCC (State Grid Corporation of China) 
- 42,372 consumers
- 1,035 days of daily consumption (01/01/2014 - 10/30/2016)
- Binary labels for malicious activities
- Challenge: Highly imbalanced (~10% fraud), missing values, long time-series
### üîç Key Features
1. Production-Grade Feature Engineering
- TSFRESH extracts 700+ features: autocorrelation, entropy, trend, seasonality
- Domain features: Zero-consumption streaks, sudden drop detection, weekend/weekday pattern disruption
- Missing value handling: Forward-fill ‚Üí Backward-fill ‚Üí Zero imputation per consumer
2. Hybrid Detection
- Isolation Forest: Catches novel theft patterns (unsupervised)
- XGBoost: High-precision classification with class imbalance handling (SMOTETomek)
- SHAP explainability: Feature importance for every prediction
3. Edge-Optimized SLM
- 4-bit GGUF quantization: Phi-3-mini runs on 4GB RAM
- Structured prompts: No token inflation from raw time-series text
- Template fallback: Graceful degradation if LLM unavailable
4. MLOps & Monitoring
- Drift Detection: PSI + KS-test for seasonal changes, infrastructure updates
- CI/CD: GitHub Actions with automated retraining triggers
- Containerized: Docker images for API and training pipelines
5. Monitoring Dashboard
- Metrics: Tracked via Prometheus/Grafana.
- Alerting: Slack/Email triggers when PSI > 0.2.

# üõ†Ô∏è Configuration
Edit config/pipeline_config.yaml:
```bash
feature_extraction:
  tsfresh_settings: "EfficientFCParameters"  # or "ComprehensiveFCParameters"
  n_jobs: 4
  
detection:
  isolation_forest:
    contamination: 0.05
    n_estimators: 100
  xgboost:
    max_depth: 6
    learning_rate: 0.05
    scale_pos_weight: 10  # For class imbalance

llm:
  model: "phi-3-mini-4k-instruct"
  quantization: "Q4_K_M"  # 4-bit quantization
  max_tokens: 256
  temperature: 0.3  # Low for factual reports

monitoring:
  psi_threshold: 0.2  # Alert if >0.2
  check_frequency: "daily"
```

# üê≥ Deployment
Docker (Recommended)
```bash
# Build images
docker build -f docker/Dockerfile.api -t theft-detection-api .
docker build -f docker/Dockerfile.training -t theft-detection-training .

# Run API
docker run -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -e MODEL_PATH=/app/models/hybrid_detector.joblib \
  theft-detection-api
```  
# Kubernetes
```bsah
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
kubectl apply -f k8s/hpa.yaml  # Auto-scaling
```
# üìö Documentation
- Architecture Deep Dive
- API Reference
- Deployment Guide
- MLOps Setup


# üôè Acknowledgments
- SGCC Dataset: State Grid Corporation of China
- TSFRESH: Blue Yonder GmbH
- Hybrid Architecture: Based on "A Hybrid Machine Learning Framework for Electricity Fraud Detection" 


## Key Production-Ready Elements

1. **Efficient Architecture**: Uses TSFRESH for feature extraction (not raw text), Isolation Forest for anomaly scoring, XGBoost for classification, and SLM **only** for report generation.

2. **Quantization**: Includes GGUF/AWQ quantization scripts for edge deployment on utility company servers.

3. **Drift Monitoring**: Implements PSI (Population Stability Index) and KS-tests to detect seasonal changes and infrastructure updates.

4. **CI/CD**: GitHub Actions workflow with automated testing, training, drift detection, and containerized deployment.

5. **Explainability**: SHAP integration for every prediction, enabling transparency for field inspection teams.

6. **Class Imbalance Handling**: SMOTETomek resampling and XGBoost `scale_pos_weight` for the highly imbalanced SGCC dataset.

This architecture avoids the "textualizing time-series" anti-pattern while delivering the interpretability benefits of SLMs through structured prompting.
